# gpt2-chinese-novel
经过训练的一个82M的中文GPT2模型，使用BERT的Tokenizer。中文语料采用后宫甄嬛传以及庆余年小说，大小约14M。训练30个周期，batchsize=8。最终可以续写10句以上的连续文字。
